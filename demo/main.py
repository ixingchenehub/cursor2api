import base64
import json
import os
import shutil
import subprocess
import tempfile
import time
from typing import Optional

# CRITICAL: Load .env file BEFORE importing any config
import load_env  # This must be the first import to load environment variables

from curl_cffi import AsyncSession, Response
from fastapi import FastAPI, Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from loguru import logger
from starlette.middleware.cors import CORSMiddleware

from app.config import SCRIPT_URL, FP, API_KEY, MODELS, SYSTEM_PROMPT_INJECT, TIMEOUT, PROXY, USER_PROMPT_INJECT, \
    X_IS_HUMAN_SERVER_URL, ENABLE_FUNCTION_CALLING, RESILIENCE_MAX_RETRIES, MAX_TOKENS, \
    COMPRESSION_ENABLED, COMPRESSION_RESERVE_RATIO
from app.errors import CursorWebError
from app.fingerprint_service import FingerprintGenerator, encode_fingerprint_to_base64
from app.models import ChatCompletionRequest, Message, ModelsResponse, Model, Usage, OpenAIMessageContent, ToolCall, \
    FingerprintRequest, FingerprintResponse
from app.resilience import get_resilience_engine
from app.utils import error_wrapper, to_async, generate_random_string, non_stream_chat_completion, \
    stream_chat_completion, safe_stream_wrapper, match_tool_name
from app.message_compressor import get_message_compressor

main_code = open('./jscode/main.js', 'r', encoding='utf-8').read()
env_code = open('./jscode/env.js', 'r', encoding='utf-8').read()
app = FastAPI()

security = HTTPBearer()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/v1/chat/completions")
async def chat_completions(
        request: ChatCompletionRequest,
        credentials: HTTPAuthorizationCredentials = Depends(security),
):
    """å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚"""

    if credentials.credentials != API_KEY:
        raise HTTPException(401, 'api key é”™è¯¯')

    chat_generator = cursor_chat(request)
    # async for c in chat_generator:
    #     logger.debug(c)

    if request.stream:
        return await error_wrapper(safe_stream_wrapper, stream_chat_completion, request, chat_generator)
    else:
        return await error_wrapper(non_stream_chat_completion, request, chat_generator)



@app.get("/metrics/resilience")
async def get_resilience_metrics(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """
    Expose resilience engine metrics for monitoring.
    Returns real-time statistics about retries, circuit breaker state, and rate limiting.
    """
    if credentials.credentials != API_KEY:
        raise HTTPException(401, 'api key é”™è¯¯')
    
    resilience_engine = get_resilience_engine()
    return resilience_engine.get_metrics()


@app.post("/v1/fingerprint/generate", response_model=FingerprintResponse)
async def generate_fingerprint(
    request: FingerprintRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security),
):
    """Generate browser fingerprint based on specified mode
    
    æ ¹æ®æŒ‡å®šçš„æ¨¡å¼ç”Ÿæˆæµè§ˆå™¨æŒ‡çº¹ã€‚æ”¯æŒå››ç§ç”Ÿæˆæ¨¡å¼:
    - current: ä½¿ç”¨ç¯å¢ƒå˜é‡FPé…ç½®ç”ŸæˆæŒ‡çº¹
    - desktop: éšæœºç”Ÿæˆæ¡Œé¢ç«¯æµè§ˆå™¨æŒ‡çº¹
    - mobile: éšæœºç”Ÿæˆç§»åŠ¨ç«¯æµè§ˆå™¨æŒ‡çº¹
    - any: ä»æ‰€æœ‰æŒ‡çº¹æ¨¡æ¿ä¸­éšæœºé€‰æ‹©
    
    Args:
        request: Fingerprint generation request with mode selection
        credentials: Bearer token for authentication
        
    Returns:
        FingerprintResponse with complete fingerprint object and base64 encoded string
        
    Raises:
        HTTPException: 401 if authentication fails, 500 if generation fails
    """
    # Validate authentication
    if credentials.credentials != API_KEY:
        raise HTTPException(status_code=401, detail='api key é”™è¯¯')
    
    try:
        # Initialize generator with environment FP
        generator = FingerprintGenerator(env_fp=FP)
        
        # Generate fingerprint based on mode
        fingerprint = generator.generate(mode=request.mode)
        
        # Encode to base64
        base64_fp = encode_fingerprint_to_base64(fingerprint)
        
        # Return response
        return FingerprintResponse(
            fingerprint=fingerprint,
            base64=base64_fp
        )
    except Exception as e:
        logger.error(f"Fingerprint generation failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f'æŒ‡çº¹ç”Ÿæˆå¤±è´¥: {str(e)}'
        )


@app.get("/v1/models")
async def list_models(credentials: HTTPAuthorizationCredentials = Depends(security)):
    if credentials.credentials != API_KEY:
        raise HTTPException(401, 'api key é”™è¯¯')
    
    models = MODELS.split(',')
    model_list = []

    for model_id in models:
        model_list.append(
            Model(
                id=model_id,  # ä½¿ç”¨model nameä½œä¸ºå¯¹å¤–çš„id
                object="model",
                created=int(time.time()),
                owned_by='',
            )
        )

    return ModelsResponse(object="list", data=model_list)


def inject_system_prompt(list_openai_message: list[Message], inject_prompt: str):
    # æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨systemè§’è‰²çš„æ¶ˆæ¯
    system_message_found = False

    for message in list_openai_message:
        if message.role == "system":
            system_message_found = True
            # å¤„ç†contentå­—æ®µï¼Œéœ€è¦è€ƒè™‘ä¸åŒçš„æ•°æ®ç±»å‹
            if message.content is None:
                message.content = inject_prompt
            elif isinstance(message.content, str):
                message.content += f'\n{inject_prompt}'
            elif isinstance(message.content, list):
                # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œéœ€è¦æ‰¾åˆ°textç±»å‹çš„å†…å®¹è¿›è¡Œè¿½åŠ 
                # æˆ–è€…æ·»åŠ ä¸€ä¸ªæ–°çš„textå†…å®¹é¡¹
                text_content_found = False
                for content_item in message.content:
                    if content_item.type == "text" and content_item.text:
                        content_item.text += f'\n{inject_prompt}'
                        text_content_found = True
                        break

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°textå†…å®¹ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„textå†…å®¹é¡¹
                if not text_content_found:
                    new_text_content = OpenAIMessageContent(
                        type="text",
                        text=inject_prompt
                        , image_url=None)
                    message.content.append(new_text_content)
            break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªsystemæ¶ˆæ¯åå°±é€€å‡ºå¾ªç¯

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°systemæ¶ˆæ¯ï¼Œåœ¨åˆ—è¡¨å¼€å¤´æ’å…¥ä¸€ä¸ªæ–°çš„systemæ¶ˆæ¯
    if not system_message_found:
        system_message = Message(
            role="system",
            content=inject_prompt
            , tool_call_id=None, tool_calls=None)
        list_openai_message.insert(0, system_message)


def collect_developer_messages(list_openai_message: list[Message]) -> str:
    collected_contents = []

    # ä»åå¾€å‰éå†ï¼Œé¿å…åˆ é™¤å…ƒç´ æ—¶ç´¢å¼•å˜åŒ–çš„é—®é¢˜
    for i in range(len(list_openai_message) - 1, -1, -1):
        message = list_openai_message[i]

        if message.role == "developer":
            # æå–æ¶ˆæ¯å†…å®¹
            content_text = ""

            if message.content is None:
                content_text = ""
            elif isinstance(message.content, str):
                content_text = message.content
            elif isinstance(message.content, list):
                # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œæå–æ‰€æœ‰textç±»å‹çš„å†…å®¹
                text_parts = []
                for content_item in message.content:
                    if content_item.type == "text" and content_item.text:
                        text_parts.append(content_item.text)
                content_text = " ".join(text_parts)  # å¤šä¸ªtextå†…å®¹ç”¨ç©ºæ ¼è¿æ¥

            # å°†å†…å®¹æ·»åŠ åˆ°æ”¶é›†åˆ—è¡¨çš„å¼€å¤´ï¼Œä¿æŒåŸå§‹é¡ºåº
            collected_contents.insert(0, content_text)

            # åˆ é™¤è¯¥æ¶ˆæ¯
            list_openai_message.pop(i)

    # å°†æ”¶é›†åˆ°çš„å†…å®¹æŒ‰\næ‹¼æ¥å¹¶è¿”å›
    return "\n".join(collected_contents)


def to_cursor_messages(request: ChatCompletionRequest):
    list_openai_message: list[Message] = request.messages
    if list_openai_message is None:
        list_openai_message = []

    developer_messages = collect_developer_messages(list_openai_message)
    inject_system_prompt(list_openai_message, developer_messages)

    if ENABLE_FUNCTION_CALLING:
        if request.tools:
            tools = [tool.model_dump_json() for tool in request.tools]
            inject_system_prompt(list_openai_message, "ä½ å¯ç”¨çš„å·¥å…·: " + json.dumps(tools))
            inject_system_prompt(list_openai_message, "ä¸å…è®¸ä½¿ç”¨tool_calls: xxxxè°ƒç”¨å·¥å…·ï¼Œè¯·ä½¿ç”¨åŸç”Ÿçš„å·¥å…·è°ƒç”¨æ–¹æ³•")

    if SYSTEM_PROMPT_INJECT:
        inject_system_prompt(list_openai_message, SYSTEM_PROMPT_INJECT)
    if USER_PROMPT_INJECT:
        list_openai_message.append(Message(role='user', content=USER_PROMPT_INJECT, tool_calls=None, tool_call_id=None))

    result: list[dict[str, str]] = []

    for m in list_openai_message:
        if not m:
            continue

        if ENABLE_FUNCTION_CALLING:
            if m.tool_calls:
                message = {
                    'role': m.role,
                    'parts': [{
                        'type': 'text',
                        'text': f"tool_calls: {json.dumps(m.tool_calls, ensure_ascii=False)}"
                    }]
                }
                result.append(message)
                continue

            if m.tool_call_id:
                message = {
                    'role': 'user',
                    'parts': [{
                        'type': 'text',
                        'text': f"{m.role}: tool_call_id: {m.tool_call_id} {m.content}"
                    }]
                }
                result.append(message)
                continue

        text = ''
        if m.content is None:
            text = ''
        elif isinstance(m.content, str):
            text = m.content
        else:
            for content in m.content:
                if not content.text:
                    continue
                text = text + content.text
        message = {
            'role': m.role,
            'parts': [{
                'type': 'text',
                'text': text
            }]
        }
        result.append(message)

    if result[0]['role'] == 'system' and not result[0]['parts'][0]['text']:
        result.pop(0)

    return result


def parse_sse_line(line: str) -> Optional[str]:
    """è§£æSSEæ•°æ®è¡Œ"""
    line = line.strip()
    if line.startswith("data: "):
        return line[6:]  # å»æ‰ 'data: ' å‰ç¼€
    return None


async def cursor_chat(request: ChatCompletionRequest):
    # æå–å¯ç”¨å·¥å…·ååˆ—è¡¨ï¼Œç”¨äºåç»­ä¿®æ­£
    available_tool_names = []
    if ENABLE_FUNCTION_CALLING and request.tools:
        available_tool_names = [tool.function.name for tool in request.tools]

    # Apply message compression if enabled to prevent token overflow
    messages_to_send = request.messages
    if COMPRESSION_ENABLED and messages_to_send:
        try:
            compressor = get_message_compressor(model_name=request.model)
            compressed_messages, stats = compressor.compress(
                messages=messages_to_send,
                max_tokens=MAX_TOKENS,
                reserve_ratio=COMPRESSION_RESERVE_RATIO
            )
            
            if stats["level"] > 0:
                logger.warning(
                    f"ğŸ—œï¸  Message compression applied: "
                    f"Level {stats['level']}, "
                    f"Tokens: {stats['original_tokens']} â†’ {stats['final_tokens']} "
                    f"({stats['compression_ratio']*100:.1f}% reduced), "
                    f"Removed {stats['removed_messages']} messages"
                )
                messages_to_send = compressed_messages
            else:
                logger.debug(f"âœ… No compression needed: {stats['original_tokens']} tokens within limit")
        except Exception as e:
            logger.error(f"âŒ Message compression failed: {e}. Using original messages.")

    # Create a modified request with compressed messages
    request_with_compressed_messages = ChatCompletionRequest(
        messages=messages_to_send,
        model=request.model,
        stream=request.stream,
        tools=request.tools
    )

    json_data = {
        "context": [

        ],
        "model": request.model,
        "id": generate_random_string(16),
        "messages": to_cursor_messages(request_with_compressed_messages),
        "trigger": "submit-message"
    }
    async with AsyncSession(impersonate='chrome', timeout=TIMEOUT, proxy=PROXY) as session:
            if X_IS_HUMAN_SERVER_URL:
                x_is_human = await get_x_is_human_server(session)
            else:
                x_is_human = await get_x_is_human(session)
            logger.debug(x_is_human)
            headers = {
                'User-Agent': FP.get("userAgent"),
                # 'Accept-Encoding': 'gzip, deflate, br, zstd',
                'Content-Type': 'application/json',
                'sec-ch-ua-platform': '"Windows"',
                'x-path': '/api/chat',
                'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
                'x-method': 'POST',
                'sec-ch-ua-bitness': '"64"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-arch': '"x86"',
                'x-is-human': x_is_human,
                'sec-ch-ua-platform-version': '"19.0.0"',
                'origin': 'https://cursor.com',
                'sec-fetch-site': 'same-origin',
                'sec-fetch-mode': 'cors',
                'sec-fetch-dest': 'empty',
                'referer': 'https://cursor.com/en-US/learn/how-ai-models-work',
                'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'priority': 'u=1, i',
            }
            logger.debug(json_data)
            async with session.stream("POST", 'https://cursor.com/api/chat', headers=headers, json=json_data,
                                      impersonate='chrome') as response:
                response: Response
                # logger.debug(await response.atext())

                if response.status_code != 200:
                    text = await response.atext()
                    if 'Attention Required! | Cloudflare' in text:
                        text = 'Cloudflare 403'
                    raise CursorWebError(response.status_code, text)
                content_type = response.headers['content-type']
                if 'text/event-stream' not in content_type:
                    text = await response.atext()
                    raise CursorWebError(response.status_code, "å“åº”éäº‹ä»¶æµ: " + text)
                async for line in response.aiter_lines():
                    line = line.decode("utf-8")
                    logger.debug(line)
                    data = parse_sse_line(line)
                    if not data:
                        continue
                    if data and data.strip():
                        try:
                            event_data = json.loads(data)
                            if event_data.get('type') == 'error':
                                err_msg = event_data.get('errorText', 'errorTextä¸ºç©º')
                                if 'The content field in the Message object at' in err_msg:
                                    err_msg = "æ¶ˆæ¯ä¸ºç©ºï¼Œå¾ˆå¯èƒ½ä½ çš„æ¶ˆæ¯åªåŒ…å«å›¾ç‰‡ï¼Œæœ¬æ¥å£ä¸æ”¯æŒå›¾ç‰‡\n" + err_msg
                                raise CursorWebError(response.status_code, err_msg)
                            if event_data.get('type') == 'finish':
                                usage = event_data.get('messageMetadata', {}).get('usage')
                                if not usage:
                                    continue
                                yield Usage(prompt_tokens=usage.get('inputTokens'),
                                            completion_tokens=usage.get('outputTokens'),
                                            total_tokens=usage.get('totalTokens'))
                                return
                            if ENABLE_FUNCTION_CALLING:
                                if event_data.get('type') == 'tool-input-error':
                                    tool_call_id = event_data.get('toolCallId')
                                    tool_name = event_data.get('toolName')
                                    tool_input = event_data.get('input')
                                    if isinstance(tool_input, str):
                                        tool_input_str = tool_input
                                    else:
                                        tool_input_str = json.dumps(tool_input)

                                    # ä¿®æ­£å·¥å…·åç§°
                                    if available_tool_names:
                                        tool_name = match_tool_name(tool_name, available_tool_names)

                                    response.close()  # å·¥å…·è¿”å›äº†ç›´æ¥ææ–­
                                    yield ToolCall(toolId=tool_call_id, toolInput=tool_input_str, toolName=tool_name)
                                    return

                            delta = event_data.get('delta')
                            # logger.debug(delta)
                            if not delta:
                                continue
                            yield delta
                        except json.JSONDecodeError:
                            continue


async def get_x_is_human_server(session: AsyncSession):
    headers = {
        'User-Agent': FP.get("userAgent"),
        # 'Accept-Encoding': 'gzip, deflate, br, zstd',
        'sec-ch-ua-arch': '"x86"',
        'sec-ch-ua-platform': '"Windows"',
        'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
        'sec-ch-ua-bitness': '"64"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform-version': '"19.0.0"',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-mode': 'no-cors',
        'sec-fetch-dest': 'script',
        'referer': 'https://cursor.com/en-US/learn/how-ai-models-work',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }

    response = await session.get(SCRIPT_URL,
                                 headers=headers,
                                 impersonate='chrome')
    cursor_js = response.text
    js_b64 = base64.b64encode(cursor_js.encode('utf-8')).decode("utf-8")

    response = await session.post(X_IS_HUMAN_SERVER_URL, json={
        "jscode": js_b64,
        "fp": FP
    })
    try:
        s = response.json().get('s')
    except json.decoder.JSONDecodeError:
        raise CursorWebError(response.status_code, 'çº¯ç®—æœåŠ¡å™¨è¿”å›ç»“æœé”™è¯¯: ' + response.text)
    if not s:
        raise CursorWebError(response.status_code, 'çº¯ç®—æœåŠ¡å™¨è¿”å›ç»“æœé”™è¯¯: ' + response.text)

    return response.text


async def get_x_is_human(session: AsyncSession):
    headers = {
        'User-Agent': FP.get("userAgent"),
        # 'Accept-Encoding': 'gzip, deflate, br, zstd',
        'sec-ch-ua-arch': '"x86"',
        'sec-ch-ua-platform': '"Windows"',
        'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
        'sec-ch-ua-bitness': '"64"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform-version': '"19.0.0"',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-mode': 'no-cors',
        'sec-fetch-dest': 'script',
        'referer': 'https://cursor.com/en-US/learn/how-ai-models-work',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }

    response = await session.get(SCRIPT_URL,
                                 headers=headers,
                                 impersonate='chrome')
    cursor_js = response.text

    # æ›¿æ¢æŒ‡çº¹ (æ·»åŠ ç©ºå€¼ä¿æŠ¤,é˜²æ­¢Noneå¯¼è‡´TypeError)
    main = (main_code.replace("$$currentScriptSrc$$", SCRIPT_URL)
            .replace("$$UNMASKED_VENDOR_WEBGL$$", FP.get("UNMASKED_VENDOR_WEBGL") or "")
            .replace("$$UNMASKED_RENDERER_WEBGL$$", FP.get("UNMASKED_RENDERER_WEBGL") or "")
            .replace("$$userAgent$$", FP.get("userAgent") or ""))

    # æ›¿æ¢ä»£ç 
    main = main.replace('$$env_jscode$$', env_code)
    main = main.replace("$$cursor_jscode$$", cursor_js)
    return await runjs(main)


@to_async
def runjs(jscode: str) -> str:
    """
    æ‰§è¡Œ JavaScript ä»£ç å¹¶è¿”å›æ ‡å‡†è¾“å‡ºå†…å®¹ã€‚

    Args:
        jscode: è¦æ‰§è¡Œçš„ JavaScript ä»£ç å­—ç¬¦ä¸²

    Returns:
        Node.js ç¨‹åºçš„æ ‡å‡†è¾“å‡ºå†…å®¹

    Raises:
        FileNotFoundError: Node.js æœªå®‰è£…æˆ–ä¸åœ¨ç³»ç»Ÿ PATH ä¸­
        subprocess.CalledProcessError: Node.js ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼Œå¼‚å¸¸ä¿¡æ¯åŒ…å« stdout å’Œ stderr
    """
    temp_dir = tempfile.mkdtemp()
    try:
        js_file_path = os.path.join(temp_dir, "script.js")
        with open(js_file_path, "w", encoding="utf-8") as f:
            f.write(jscode)

        result = subprocess.run(
            ['node', js_file_path],
            capture_output=True,
            text=True,
            encoding="utf-8"
        )

        if result.returncode != 0:
            error_msg = f"Node.js æ‰§è¡Œå¤±è´¥ (é€€å‡ºç : {result.returncode})\nSTDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"
            logger.error(error_msg)
            raise subprocess.CalledProcessError(result.returncode, ['node', js_file_path], result.stdout, result.stderr)

        return result.stdout.strip()
    finally:
        shutil.rmtree(temp_dir)


def print_startup_banner():
    """Print a modern, colorful startup banner with system information"""
    from datetime import datetime
    import platform
    
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â•‘
â•‘  â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â•‘
â•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â•‘
â•‘  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘  â•‘
â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â•‘
â•‘   â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•  â•‘
â•‘                                                                              â•‘
â•‘                      ğŸš€ FastAPI Cursor Web Proxy Service                     â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    
    logger.info(banner)
    logger.info("â”" * 80)
    logger.info("ğŸ“‹ System Information")
    logger.info("â”" * 80)
    logger.info(f"  ğŸ–¥ï¸  Platform      : {platform.system()} {platform.release()}")
    logger.info(f"  ğŸ Python        : {platform.python_version()}")
    logger.info(f"  ğŸ“… Start Time    : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"  ğŸŒ Host          : 0.0.0.0:8000")
    logger.info("â”" * 80)
    logger.info("ğŸ”§ Configuration")
    logger.info("â”" * 80)
    logger.info(f"  ğŸ“¦ Models        : {len(MODELS.split(','))} available")
    logger.info(f"  â±ï¸  Timeout       : {TIMEOUT}s")
    logger.info(f"  ğŸ”„ Max Retries   : {RESILIENCE_MAX_RETRIES}")
    logger.info(f"  ğŸ” Auth          : {'Enabled âœ“' if API_KEY else 'Disabled âœ—'}")
    logger.info(f"  ğŸ›¡ï¸  Proxy         : {'Enabled âœ“' if PROXY else 'Disabled âœ—'}")
    logger.info(f"  ğŸ¤– Functions     : {'Enabled âœ“' if ENABLE_FUNCTION_CALLING else 'Disabled âœ—'}")
    logger.info("â”" * 80)
    logger.info("ğŸ“¡ Available Endpoints")
    logger.info("â”" * 80)
    logger.info("  POST   /v1/chat/completions       - Chat completion endpoint")
    logger.info("  POST   /v1/fingerprint/generate   - Browser fingerprint generator")
    logger.info("  GET    /v1/models                 - List available models")
    logger.info("  GET    /metrics/resilience        - Resilience metrics")
    logger.info("â”" * 80)
    logger.info("âœ¨ Service is ready to accept requests!")
    logger.info("â”" * 80)


if __name__ == "__main__":
    import uvicorn
    
    # Print startup banner before starting server
    print_startup_banner()
    
    # Configure uvicorn with custom log config to suppress default logs
    log_config = uvicorn.config.LOGGING_CONFIG
    log_config["formatters"]["access"]["fmt"] = "%(levelprefix)s %(client_addr)s - \"%(request_line)s\" %(status_code)s"
    log_config["formatters"]["default"]["fmt"] = "%(levelprefix)s %(message)s"
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="warning",  # Suppress INFO logs from uvicorn
        access_log=False,  # Disable access logs
        log_config=log_config,
    )
